tosca_definitions_version: cloudify_dsl_1_3


description: >
  Example blueprint for AWS EMR functionality.
  
  Prerequisites:
  1. S3 bucket for cluster logging (emr_cluster_s3_log_path)
  2. AWS API credentials (aws_access_key_id & aws_secret_access_key)
  
  This will spin up a new EMR cluster with a MASTER node and 1 or more
  CORE nodes. It will have a couple of bootstrap actions, 1 to
  install Node.js and 1 to copy a custom UDF (or whatever you specify
  in emr_custom_hive_udf_path) to the cluster. Then it will
  start a new cluster step (process) and run the example
  WordCount step from Amazon.
  
  To scale the number of CORE nodes in the cluster, run the
  scale_emr_cluster_instance_group workflow. Note, scaling
  can only work if there was an instance group created with the
  cluster at deployment time. AWS does not allow for creating
  cluster instance groups at run-time. 


imports:
- http://www.getcloudify.org/spec/cloudify/3.4.1/types.yaml
- ../plugin.yaml


inputs:
  aws_access_key_id:
    type: string
  
  aws_secret_access_key:
    type: string
    
  aws_region_name:
    type: string
    default: us-east-1
  
  emr_cluster_name:
    description: >
      Name of the EMR cluster to create
    type: string
    default: Cloudify Hadoop Cluster
  
  emr_cluster_ec2_keyname:
    description: >
      Name of AWS EC2 keypair to use for the EMR cluster
    type: string
    default: hostpool-test
    
  emr_cluster_s3_log_path:
    description: >
      S3 path to a folder to hold EMR cluster logs
    type: string
    
  emr_cluster_core_instances:
    description: >
      Number of CORE cluster instances to spawn on create
    type: integer
    default: 1
    
  emr_custom_hive_udf_path:
    description: >
      Custom UDF to fetch and place on cluster
    type: string
    default: s3://01000101-hadoop-scripts/udfs/UDFStrToDate.java


dsl_definitions:
  aws_config: &aws_config
    aws_access_key_id: { get_input: aws_access_key_id }
    aws_secret_access_key: { get_input: aws_secret_access_key }
    ec2_region_name: { get_input: aws_region_name }


node_templates:
  #emr_cluster:
  #  type: cloudify.aws.nodes.emr.Cluster
  #  properties:
  #    resource_id: j-28MTL6RV3L6FX
  #    use_external_resource: true
  #    aws_config: *aws_config

  emr_cluster:
    type: cloudify.aws.nodes.emr.Cluster
    properties:
      name: { get_input: emr_cluster_name }
      log_uri: { get_input: emr_cluster_s3_log_path }
      ec2_keyname: { get_input: emr_cluster_ec2_keyname }
      applications: [Hive, Pig, Ganglia]
      aws_config: *aws_config
    relationships:
    - target: bootstrap_copy_hive_udf
      type: cloudify.aws.relationships.emr.cluster_connected_to_bootstrap_action
    - target: bootstrap_install_nodejs
      type: cloudify.aws.relationships.emr.cluster_connected_to_bootstrap_action
    - target: instance_group_master
      type: cloudify.aws.relationships.emr.cluster_connected_to_instance_group
    - target: instance_group_core
      type: cloudify.aws.relationships.emr.cluster_connected_to_instance_group
    
  ###
  # Cluster Instance Groups
  ###
  instance_group_master:
    type: cloudify.aws.nodes.emr.InstanceGroup
    properties:
      name: Master node
      role: MASTER
      type: m1.medium
      
  instance_group_core:
    type: cloudify.aws.nodes.emr.InstanceGroup
    properties:
      name: Core node
      role: CORE
      num_instances: { get_input: emr_cluster_core_instances }

  ###
  # Cluster Bootstrap Actions
  ###
  # Use local scripts to fetch external resources in S3
  bootstrap_copy_hive_udf:
    type: cloudify.aws.nodes.emr.BootstrapAction
    properties:
      name: "Copy Custom Hive UDF"
      path: "file:///usr/bin/aws"
      bootstrap_action_args:
      - s3
      - cp
      - { get_input: emr_custom_hive_udf_path }
      - "/home/hadoop/"

  # Run a bootstrap action directly from S3
  bootstrap_install_nodejs:
    type: cloudify.aws.nodes.emr.BootstrapAction
    properties:
      name: "Install Node.js"
      path: "s3://01000101-hadoop-scripts/node/install-nodejs.sh"

  ###
  # Cluster Steps
  ###
  # This is the traditional WordCount EMR example step from Amazon
  step_wordcount:
    type: cloudify.aws.nodes.emr.StreamingStep
    properties:
      name: Sample Word Count
      input: "s3://elasticmapreduce/samples/wordcount/input"
      mapper: wordSplitter.py
      reducer: aggregate
      output: "s3://cloudify-emr-logs-j8dxz/output"
      step_args:
      - hadoop-streaming
      - "-files"
      - "s3://elasticmapreduce/samples/wordcount/wordSplitter.py"
      aws_config: *aws_config
    relationships:
    - target: emr_cluster
      type: cloudify.aws.relationships.emr.step_contained_in_cluster
